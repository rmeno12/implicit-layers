{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit Layers\n",
    "Normally, layers are expressed as an application of some function $f: \\mathcal{X} \\to \\mathcal{Z}$. That is\n",
    "\\begin{equation*}\n",
    "    z = f(x)\n",
    "\\end{equation*}\n",
    "Instead, we want to consider an implicit function $g: \\mathcal{X} \\times \\mathcal{Z} \\to \\mathbb{R}^n$, where the output $z$ of the layer is constrained to some root of $g$ such that \n",
    "\\begin{equation*}\n",
    "    g(x, z) = 0\n",
    "\\end{equation*}\n",
    "\n",
    "Advantages of this method are that we do not need any way to actually compute $g$, which we would if this were an explicit layer. More fundamentally, this separates the computation of the layer from the definition of the layer. They are also far less memory intensive, as there is no hidden state which to store to perform backpropogation. Indeed, gradients can be computed directly from the implicit function theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed point iteration layer\n",
    "Suppose we have inputs, and outputs $x, z \\in \\mathbb{R}^n$ and model weights $W \\in \\mathbb{R}^{n\\times n}$. Then consider a fixed point iteration layer such that we initialize $z := 0$ and repeat the following until convergence:\n",
    "\\begin{equation*}\n",
    "z := \\tanh{(Wz + x)}\n",
    "\\end{equation*}\n",
    "\n",
    "Eventually, we may reach some fixed output $z^*$ such that $z^* = \\tanh{(Wz^* + x)}$. In this case, the implicit representation is as follows.\n",
    "\\begin{equation*}\n",
    "g(x, z) = z - \\tanh{(Wz + x)}\n",
    "\\end{equation*}\n",
    "For now, we will assume that for typical values of $W$ that convergence is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class TanhFixedPointLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, out_features: int, tol: float = 1e-4, max_iter: int = 50\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(out_features, out_features, bias=False).to(device)\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        z = torch.zeros_like(x).to(device)\n",
    "        self.iterations = 0\n",
    "\n",
    "        while self.iterations < self.max_iter:\n",
    "            z_next = torch.tanh(self.linear(z) + x)\n",
    "            self.err = torch.norm(z - z_next)\n",
    "            z = z_next\n",
    "            self.iterations += 1\n",
    "            if self.err < self.tol:\n",
    "                break\n",
    "\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminated after 14 iterations with error 4.8777183110360056e-05\n"
     ]
    }
   ],
   "source": [
    "layer = TanhFixedPointLayer(50)\n",
    "X = torch.randn(10, 50).to(device)\n",
    "Z = layer(X)\n",
    "print(f\"terminated after {layer.iterations} iterations with error {layer.err}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this layer on MNIST data instead of random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mnist_train = datasets.MNIST(\n",
    "    \"data/\", train=True, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "mnist_test = datasets.MNIST(\n",
    "    \"data/\", train=False, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "train_loader = DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=100, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(784, 100),\n",
    "    TanhFixedPointLayer(100, max_iter=200),\n",
    "    nn.Linear(100, 10),\n",
    ").to(device)\n",
    "opt = optim.SGD(model.parameters(), lr=1e-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "from typing import Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def epoch(\n",
    "    loader: DataLoader, model: nn.Module, opt: optim.Optimizer = None, monitor=None\n",
    ") -> Tuple[float, float, float]:\n",
    "    total_loss, total_err, total_monitor = 0.0, 0.0, 0.0\n",
    "    model.eval() if opt is None else model.train()\n",
    "\n",
    "    for X, y in tqdm(loader, leave=False):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        yp = model(X)\n",
    "        loss = nn.CrossEntropyLoss()(yp, y)\n",
    "        if opt:\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            if sum(torch.sum(torch.isnan(p.grad)) for p in model.parameters()) == 0:\n",
    "                opt.step()\n",
    "\n",
    "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
    "        total_loss += loss.item() * X.shape[0]\n",
    "        if monitor is not None:\n",
    "            total_monitor += monitor(model)\n",
    "\n",
    "    return (\n",
    "        total_err / len(loader.dataset),\n",
    "        total_loss / len(loader.dataset),\n",
    "        total_monitor / len(loader),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02feb94fd1604268b34297805f514c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4638813b914e2290b9d88d724fe029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.1113, Loss: 0.4034, FP Iters: 53.40 | Test Error: 0.0717, Loss: 0.2418, FP Iters: 56.55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d4de0ab04d45bca60591bf30b39847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e131ecbe6274993880b42fdaa5ec1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0574, Loss: 0.1941, FP Iters: 53.09 | Test Error: 0.0498, Loss: 0.1639, FP Iters: 53.86\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4100075c78746b0a83af39618d68f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf27ff9255e436fa04f65cc7bc12cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0437, Loss: 0.1479, FP Iters: 56.71 | Test Error: 0.0459, Loss: 0.1469, FP Iters: 57.44\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ab1603fe2a425786203de4b50bc9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58eef4236a224ab58ec1e383defb9e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0364, Loss: 0.1223, FP Iters: 64.10 | Test Error: 0.0384, Loss: 0.1335, FP Iters: 60.41\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a24ff25c534e1d8a5f9d7fdeef56ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f23bfae74e54779970e3ee2e72d5c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0312, Loss: 0.1044, FP Iters: 77.47 | Test Error: 0.0379, Loss: 0.1218, FP Iters: 83.96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884851b621fe4a839d958021d9fe5fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81d4a58ab214b258b3efb0ab10f4c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0214, Loss: 0.0743, FP Iters: 80.36 | Test Error: 0.0318, Loss: 0.1057, FP Iters: 78.79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b97abd4c654e229bf2cd7451746169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861ce81e96084db6bc5805d6ad598634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0196, Loss: 0.0685, FP Iters: 76.08 | Test Error: 0.0313, Loss: 0.1033, FP Iters: 74.38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315b0a3b13cb4ee0b7e06c7d9f4a6f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb0f3aa6634490381477dacce533982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0187, Loss: 0.0655, FP Iters: 77.63 | Test Error: 0.0306, Loss: 0.1051, FP Iters: 75.75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c20195b2ca4eeca623d0d9124abba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516926d31c53407e9a59337988f28d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0180, Loss: 0.0633, FP Iters: 79.81 | Test Error: 0.0300, Loss: 0.1045, FP Iters: 78.45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fa23c30f164a6ba0fe8ad8cd057251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e85183cd48c4c63950e876265be4b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0174, Loss: 0.0612, FP Iters: 80.81 | Test Error: 0.0304, Loss: 0.1035, FP Iters: 79.67\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    if i == 5:\n",
    "        opt.param_groups[0][\"lr\"] = 1e-2\n",
    "\n",
    "    train_err, train_loss, train_fpiter = epoch(\n",
    "        train_loader, model, opt, lambda x: x[2].iterations\n",
    "    )\n",
    "    test_err, test_loss, test_fpiter = epoch(\n",
    "        test_loader, model, monitor=lambda x: x[2].iterations\n",
    "    )\n",
    "    print(\n",
    "        f\"Train Error: {train_err:.4f}, Loss: {train_loss:.4f}, FP Iters: {train_fpiter:.2f} | \"\n",
    "        + f\"Test Error: {test_err:.4f}, Loss: {test_loss:.4f}, FP Iters: {test_fpiter:.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is kinda disappointing. We achieve passable results, but at a considerable speed loss. Since so many iterations are being done, we're essentially running a 50-80 layer MLP to get performance on part with a single layer one. However, there are key differences (one of which is lack of exploding gradient) that will become more clear once we introduce a few more ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative root-finding techniques\n",
    "Above, we simply iterated the equation that defined the layer. However, we can use a faster root-finding method, such as Newton's method, to improve upond this dramatically. For some function $g: \\mathbb{R}^n \\to \\mathbb{R}^n$, we want to find the root $g(z) = 0$. Then, we can do the update\n",
    "\\begin{equation*}\n",
    "z := z - \\left( \\frac{\\partial g}{\\partial z}\\right)^{-1} g(z)\n",
    "\\end{equation*}\n",
    "where $\\frac{\\partial g}{\\partial z}$ is the Jacobian of $g$ with respect to $z$. We could use automatic differentiation here, but it is simple to express the Jacobian in closed form here.\n",
    "\\begin{gather*}\n",
    "g(x, z) = z - \\tanh{(Wz + x)}\\\\\n",
    "\\frac{\\partial g}{\\partial z} = I - \\text{diag}(\\text{sech}^2(Wz + x))W\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanhNewtonLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, out_features: int, tol: float = 1e-4, max_iter: int = 50\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(out_features, out_features, bias=False).to(device)\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        z = torch.tanh(x)\n",
    "        self.iterations = 0\n",
    "\n",
    "        while self.iterations < self.max_iter:\n",
    "            z_linear = self.linear(z) + x\n",
    "            g = z - torch.tanh(z_linear)\n",
    "            self.err = torch.norm(g)\n",
    "            if self.err < self.tol:\n",
    "                break\n",
    "\n",
    "            J = (\n",
    "                torch.eye(z.shape[1]).to(device)[None, :, :]\n",
    "                - (1 / torch.cosh(z_linear) ** 2)[:, :, None]\n",
    "                * self.linear.weight[None, :, :]\n",
    "            )\n",
    "            z = z - torch.linalg.solve(J, g)\n",
    "            self.iterations += 1\n",
    "\n",
    "        g = z - torch.tanh(self.linear(z) + x)\n",
    "        z[torch.norm(g, dim=1) > self.tol, :] = 0\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters: 3, err: 9.452685389987892e-07\n"
     ]
    }
   ],
   "source": [
    "layer = TanhNewtonLayer(50)\n",
    "X = torch.randn(10, 50).to(device)\n",
    "Z = layer(X)\n",
    "print(f\"iters: {layer.iterations}, err: {layer.err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This converges faster, but note that we are solving a linear system at every iteration, which is significantly more expensive. We can still plug it into the same training loop as before and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399b630db8ab48cfa1dd65a79f4c2273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e801dc197153429da822f34b1bb75af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.1153, Loss: 0.4183, Newton Iters: 6.91 | Test Error: 0.0769, Loss: 0.2601, Newton Iters: 6.83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bce402ee80425994c41895d1557c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1a462f1fef417c802a5d0d801a195e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0606, Loss: 0.2084, Newton Iters: 6.77 | Test Error: 0.0523, Loss: 0.1727, Newton Iters: 6.84\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab97ce06c084d57a21b37c97b04fdfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d005a2909127451bbf39ae621bf73f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0474, Loss: 0.1597, Newton Iters: 6.93 | Test Error: 0.0472, Loss: 0.1543, Newton Iters: 6.66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b4e97a9e8644ad94efc7f84c6a4d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b6275d08a64b0ebe61a167f1ed68c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0389, Loss: 0.1300, Newton Iters: 6.98 | Test Error: 0.0427, Loss: 0.1466, Newton Iters: 7.98\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f748854e9164a65b0a075abd72f5083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf20757a43cd4ef5b95599c2b2042835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0406, Loss: 0.1403, Newton Iters: 7.42 | Test Error: 0.0389, Loss: 0.1298, Newton Iters: 6.70\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117505c7a8f7435a8508241c07e618a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b5dab2b7a047fd9a44cd5682883358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0276, Loss: 0.0939, Newton Iters: 6.92 | Test Error: 0.0349, Loss: 0.1151, Newton Iters: 6.90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd771dff77d45f493b9bbeef82f6b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca20a8f62ffa409eba8762dd5a5c6067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0251, Loss: 0.0870, Newton Iters: 7.18 | Test Error: 0.0337, Loss: 0.1117, Newton Iters: 7.08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c7b6fbbfca41e9b30306e331714ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fab5abc84634e6b9a329b4f66a66eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0238, Loss: 0.0829, Newton Iters: 7.51 | Test Error: 0.0339, Loss: 0.1124, Newton Iters: 7.98\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(784, 100),\n",
    "    TanhNewtonLayer(100, max_iter=40),\n",
    "    nn.Linear(100, 10),\n",
    ").to(device)\n",
    "opt = optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "for i in range(8):\n",
    "    if i == 5:\n",
    "        opt.param_groups[0][\"lr\"] = 1e-2\n",
    "\n",
    "    train_err, train_loss, train_fpiter = epoch(\n",
    "        train_loader, model, opt, lambda x: x[2].iterations\n",
    "    )\n",
    "    test_err, test_loss, test_fpiter = epoch(\n",
    "        test_loader, model, monitor=lambda x: x[2].iterations\n",
    "    )\n",
    "    print(\n",
    "        f\"Train Error: {train_err:.4f}, Loss: {train_loss:.4f}, Newton Iters: {train_fpiter:.2f} | \"\n",
    "        + f\"Test Error: {test_err:.4f}, Loss: {test_loss:.4f}, Newton Iters: {test_fpiter:.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the above, this also functions. However, it is even slower despite the dramatically fewer number of iterations required. This is because it needs to invert the entire Jacobian matrix on every iteration for each sample in each minibatch. As the hidden size increases, this computation becomes increasingly expensive to the point of being intractable (taking inverses sucks). There are other quasi-Newton methods to improve on this aspect.\n",
    "\n",
    "Another issue is that since Newton's method is implemented direction inside the autograd system, the intermediate states of the Jacobian and solver also increase. It is also numerically unstable for matrices that are near-singular (this is why the NaN check exists in the eopoch function)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96e6afb47be460d864a912b73d791dca777ba706c8563dd2570f22b490a09af1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
